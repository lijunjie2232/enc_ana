TimeSeriesTransformerModel(
  (scaler): TimeSeriesMeanScaler()
  (encoder): TimeSeriesTransformerEncoder(
    (value_embedding): TimeSeriesValueEmbedding(
      (value_projection): Linear(in_features=9, out_features=64, bias=False)
    )
    (embed_positions): TimeSeriesSinusoidalPositionalEmbedding(600, 64)
    (layers): ModuleList(
      (0): TimeSeriesTransformerEncoderLayer(
        (self_attn): TimeSeriesTransformerAttention(
          (k_proj): Linear(in_features=64, out_features=64, bias=True)
          (v_proj): Linear(in_features=64, out_features=64, bias=True)
          (q_proj): Linear(in_features=64, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=64, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (activation_fn): GELUActivation()
        (fc1): Linear(in_features=64, out_features=32, bias=True)
        (fc2): Linear(in_features=32, out_features=64, bias=True)
        (final_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (1): TimeSeriesTransformerEncoderLayer(
        (self_attn): TimeSeriesTransformerAttention(
          (k_proj): Linear(in_features=64, out_features=64, bias=True)
          (v_proj): Linear(in_features=64, out_features=64, bias=True)
          (q_proj): Linear(in_features=64, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=64, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (activation_fn): GELUActivation()
        (fc1): Linear(in_features=64, out_features=32, bias=True)
        (fc2): Linear(in_features=32, out_features=64, bias=True)
        (final_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layernorm_embedding): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TimeSeriesTransformerDecoder(
    (value_embedding): TimeSeriesValueEmbedding(
      (value_projection): Linear(in_features=9, out_features=64, bias=False)
    )
    (embed_positions): TimeSeriesSinusoidalPositionalEmbedding(600, 64)
    (layers): ModuleList(
      (0): TimeSeriesTransformerDecoderLayer(
        (self_attn): TimeSeriesTransformerAttention(
          (k_proj): Linear(in_features=64, out_features=64, bias=True)
          (v_proj): Linear(in_features=64, out_features=64, bias=True)
          (q_proj): Linear(in_features=64, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=64, bias=True)
        )
        (activation_fn): GELUActivation()
        (self_attn_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): TimeSeriesTransformerAttention(
          (k_proj): Linear(in_features=64, out_features=64, bias=True)
          (v_proj): Linear(in_features=64, out_features=64, bias=True)
          (q_proj): Linear(in_features=64, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=64, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=64, out_features=32, bias=True)
        (fc2): Linear(in_features=32, out_features=64, bias=True)
        (final_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (1): TimeSeriesTransformerDecoderLayer(
        (self_attn): TimeSeriesTransformerAttention(
          (k_proj): Linear(in_features=64, out_features=64, bias=True)
          (v_proj): Linear(in_features=64, out_features=64, bias=True)
          (q_proj): Linear(in_features=64, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=64, bias=True)
        )
        (activation_fn): GELUActivation()
        (self_attn_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): TimeSeriesTransformerAttention(
          (k_proj): Linear(in_features=64, out_features=64, bias=True)
          (v_proj): Linear(in_features=64, out_features=64, bias=True)
          (q_proj): Linear(in_features=64, out_features=64, bias=True)
          (out_proj): Linear(in_features=64, out_features=64, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=64, out_features=32, bias=True)
        (fc2): Linear(in_features=32, out_features=64, bias=True)
        (final_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layernorm_embedding): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
)